\chapter{Theoretical Background}

\section{Document Understanding}
\subsection{Definition and Overview}
In general, Document Understanding is the process of ...
Traditionally, Document Understanding has been mainly concerned with the conversion of paper-based documents into a format that is readable for a computer \cite{taylor1994integrated}. 
With advancing digitization, Document Understanding covers the processing of digitally created documents as well. 

Traditionally, Document Understanding consists of an image processing step followed by a \ac{NLP} step. For both steps rule-based as well as \ac{ML} techniques exist. Paper-based documents start the Document Understanding process as scanned hard-copy of printed documents. Because these scans are essentially images representing the document at hand, in the first step the corresponding text has to be determined in a process called \ac{OCR}. During the image processing step the scanned document is converted from an image document format into a text document format. In the \ac{NLP} step the obtained text is processed to extract relevant information.

Digitally created documents already contain the information as text elements. Therefore, in principle the image processing part can be skipped. However, a different form of \ac{OCR} can still be helpful by providing layout information about the document. This is useful, because certain information tend to be more likely to be found is some regions of the document.

In summary, \ac{OCR} is needed to extract the text from document images and to transform them into unstructured text.

\ac{IE} and its sub-task \ac{NER} are used to extract relevant information from unstructured text and transform documents into a logically structured data representation.

In the following, the three central Document Understanding methods \ac{OCR}, \ac{NER}, and \ac{IE} are introduced. For each technique the history, application, and advancements through \ac{ML} are presented.
%Review traditional methods like optical character recognition (OCR), named entity recognition (NER), and information extraction (IE).
%Discuss recent advancements in machine learning and deep learning for document understanding.
%Analyze the strengths and weaknesses of existing approaches.
\subsection{Optical Character Recognition}
\ac{OCR} is the process of converting images of text into machine-encoded text for example as ASCII code. In principle, \ac{OCR} can be thought of as a computer "reading" the text contained in an image. \cite{eikvil1993optical}
Because of the various use cases that machine reading offers, \ac{OCR} has been of interest for researches for several decades. In the 1960's the first commercial \ac{OCR} machines have been released. However, theses systems have been limited to a set of standardized fonts, whose distinctive shapes have been automatically matched to the corresponding letter. 
With advances in computer hardware and and easier access to computing resources, the first \ac{OCR} systems have been released as software in the 1970's. 
The challenge for any \ac{OCR} system is to provide an accurate recognition of printed as well as hand-written text.
Typically, in traditional \ac{OCR} systems the process starts by scanning the physical document to create a digital representation. Followed by locating the regions where text can be found. In the identified areas the text components are isolated from the rest of the image. When the scanned image contains noise for example due to a low resolution or errors from the segmentation of the image, pre-processing is necessary to increase the character recognition accuracy. Pre-processing can include applying filters to smooth the edges of the digitized characters and normalization to get characters of similar size and orientation. For the actual identification of the character, various different approaches exist. This part is considered the biggest challenge of \ac{OCR}. While early systems relied on template character matching techniques which are sensitive to noise and variation, more advanced techniques use mathematical transforms (e.g. Fourier transform) of the pixel values to discover identifying characteristics. The most advanced approach is based on neural networks that have been trained with a wide range of examples to recognize patterns. When all characters and symbols are converted in final step they will be grouped together into strings based on their positions and distances. Finally, some errors can be detected and corrected by checking for incorrect spelling with a underlying dictionary.
\cite{eikvil1993optical,nagy1999optical}

Generally, \ac{OCR} is known as a rather unreliable technology and to be prone to a wide range of errors. Especially, low resolution or low contrast can lead to a higher error rate. Also, some \ac{OCR} cannot always distinguish between very similar looking characters (e.g. the letter O and the number 0). Symbols appearing less frequently are also more likely to result in errors. Finally, special typographic features like unusual fonts can cause errors as well.
\cite{nagy1999optical,baird2004robust}

To improve \ac{OCR} robustness and accuracy a number of process steps can be considered. On the one hand, advanced image processing methods can be adapted to reduce the noise in the image during pre-processing. On the other hand, in the context of Document Understanding it can be taken advantage of the observation that most documents are created in a single font. Therefore, systems can be trained specifically and adapted for the kind of document the system is supposed to handle most effectively.
A more general improvement is concerned with the approach of recognizing multiple characters instead of one by one. The concept behind this approach is that some words have a characteristic shape themselves and the detected words can be matched to a context specific dictionary. This concept in turn is the foundation of another approach as well, where the system is provided with the linguistic context, because it can be noticed that specific terms are more likely to be present in an invoice document than in a user manual for instance. \cite{nagy1999optical,baird2004robust}

The Tesseract software developed by HP and Google has been considered as one of the biggest advancements and is regarded as one of the most reliable \ac{OCR} systems of the 2000's. Tesseract is based on a \ac{LSTM} neural network that enables the system to "remember" the already processes characters to make more accurate predictions for the following character. \cite{Tesseract,hochreiter1997long}

\subsection{Information Extraction}
\ac{IE} is concerned with automatically extracting information from unstructured or semi-structured machine readable documents. \ac{IE} aims to transform unstructured text into structured data to enable computational analysis and automated reasoning. To achieve this, \ac{IE}-systems represent information written in natural language as a structured database. By extracting the logical form of the input text, \ac{IE} makes the identification of entities, relationships, and their semantic meanings within a specific domain possible. \ac{IE} is based on concepts of computer linguistics and usually uses methods from the related field of \acl{NLP}. In the context of Document Understanding, \ac{IE} plays an important role, because most documents contain mainly unstructured information and can therefore not be directly processed by machines. \cite{hobbs2010information,church1995commercial}

Just like for \ac{OCR} the roots of \ac{IE} can be traced back until the 1960's, where the first concepts of template-based \ac{IE} systems have been published. The series of seven \ac{MUC} between 1987 and 1998 are considered as a main driver behind the development of \ac{IE}. The \ac{MUC} have been focused on extracting information from news articles in a specific domain. The final two \acs{MUC} focused on \acf{NER}, which is an important sub-task of \ac{IE}. The goal of \ac{NER} is to identify meaningful objects in a text.
\cite{hobbs2010information,cowie1996information,black1998facile}

The most basic \ac{IE} approach is based on templates. An example for the template-based approach is the extraction of information that occur in a constant sequence like e-mail addresses or dates using a regular expression as template. However, this approach is quite limited if confronted with rather vague information that does not exactly fit in the categories of the underlying template. Another central challenge of \acf{IE} is the automated decision which information is relevant and which parts can be ignored without loosing important details. For those reasons, more advanced techniques from \ac{NLP} are required for more robust results.
\cite{Wilks1997InformationEA,cowie1996information}

A promising approach is the idea of so-called learning \acf{IE} systems that do not rely on set in stone templates but use either statistical techniques or \ac{ML} algorithms. 
\cite{hobbs2010information,cardie1997empirical}

\subsection{Named Entity Recognition}
A "named entity" is the name of a real-world object. Named entities can be names of persons, organizations, products, locations, etc..
\acf{NER} is concerned with identifying named entities in text and assigning them to their corresponding category. For instance, in the sentence "Steve Jobs was the founder of Apple Inc. and inventor of the iPhone" three named entities can be identified. In particular, "Steve Jobs" as a person entity, "Apple Inc." as an organization entity and finally "iPhone" as a product entity. \ac{NER} is of central importance in the field of Document Understanding, because some of the relevant information in documents fall into the category of a named entity. \cite{hobbs2010information,black1998facile}

For \ac{NER} to be successful an initial knowledge base also known as corpus in \ac{NLP} has to be set up. Typically, the corpus contains the name and type of the entity as well as information on the relation between two entities. For example, the information contained in the sentence "Mark Zuckerberg is the CEO of Meta Inc." would be stored as triplet: 

\{Person: Mark Zuckerberg, Relation: CEO, Organization: Meta Inc.\}.

A related challenge in this context is the so-called "coreference resolution", which is concerned with the case when there are multiple names for the same entity. For example, "Volkswagen" and its abbreviation "VW" should be linked to the same organization entity.
\cite{cowie1996information,cardie1997empirical}

For the 7th \ac{MUC} Black et al.\cite{black1998facile} developed a rule-based \ac{NER} system.

Besides its rigid nature, another major disadvantage of the rule-based approach is the labor intensity that goes into the manual definition of all rules. A suggested alternative are learning \ac{NER} systems that use \ac{ML} techniques and neural networks instead of set in stone rules.
\cite{hobbs2010information}

\newpage
\section{Natural Language Processing}
\subsection{Overview and History}
\ac{NLP} is concerned with the computer-based processing of information written in natural language. Typical tasks of \ac{NLP} are text classification, natural language understanding, and generation. \ac{NLP} is used for machine translation, text summarization, and chatbots.In the context of Document Understanding, \ac{NLP} provides important tools and techniques for \acf{IE}. \cite{aiello2002document}

For the first time, the automated processing and generation of natural language have been mentioned by Alan Turin in 1950 \cite{Turing1950}. Since then, \ac{NLP}-systems evolved from the initial rule-based approach to statistical concepts and finally using \ac{ML} and \acfp{ANN}. Before the 1980s, most \ac{NLP} systems relied on complex hand-written rules. Up until the early 2000s, statistical models have been the dominant method for \ac{NLP}. The first notable breakthrough of neural \ac{NLP} happened in 2003, when Benio et al. \cite{bengio} outperformed the state-of-the-art statistical language models using an \ac{ANN}.

Independent from the technical approach, any \ac{NLP}-system is based on a context specific text corpus. Text corpora are structured datasets containing large amounts of texts in a digital format. Usually, a text corpus is organized in a way to be easy to analyze and contains mainly test samples from real-world texts. For instance, academic journal repositories and Wikipedia can be seen as examples for text corpora. With a text corpus it is possible to gain information about word and letter frequencies and patterns. These information are important for the construction or training of an \ac{NLP}-system.
\cite{church1995commercial,vzivzka2019text}

\subsection{Statistical NLP}
Statistical \ac{NLP} models aim to learn the probability of specific sequences of words that occur in the underlying text corpus.
The traditional statistical approach is based on so-called n-grams, which are instances of words that are commonly grouped together. Examples for bi-grams (\textit{n = 2}) are "Artificial Intelligence" or "New York". "Natural Language Processing" is an example of an n-gram, where \textit{n = 3}. N-gram models create probability tables for the next word given the current word. Disadvantages of this approach are that n-gram models cannot capture relations between words that are outside of the current window of size \textit{n} and therefore cannot deal with unseen word pattern. For small values of \textit{n} this approach is computationally efficient. However, as \textit{n} grows, the number of potential n-grams follows an exponential growth. This effect is know as "curse of dimensionality" and can be mitigated using \acp{ANN}
\cite{bengio,vzivzka2019text}

\subsection{Neural NLP}
The advantage of \ac{NLP} models using an \ac{ANN} is that they can learn the probability of word sequences in parallel with the syntactic and semantic relationships between words. 
Because \ac{ML} algorithms and \acp{ANN} cannot directly process letters, words, or sentences, the text has to be converted into numbers. This process is called "tokenization" and has several advantages. On the one hand, it leads to a compression of the size of the dataset. On the other hand, it is possible to encode the semantical meaning into the tokenized representation. This can be achieved by representing each word as a vector. If two vectors are similar, this indicates a relationship between the words represented by them. With this approach, vectors of words sharing the same word-stem will be similar. Also, words appearing in similar contexts like \textit{go} and \textit{walk} will have high similarity measures. Vector representations that capture the semantics of the content they encode are known as "embeddings".
\cite{bengio,vzivzka2019text}

A widely adopted method for obtaining a word embedding is the "Word2vec" technique developed at Google by Mikolov et al. \cite{mikolov2013}. For this technique an \ac{ANN} is trained with a large text corpus. The "Word2vec" algorithm maps each word in the text corpus to a specific vector that can have several hundred dimensions.

The resulting word vectors can then be used again as input for specialized \acp{ANN} for example for the purpose of machine translation and summarization.

%Check more content in Len 7Neural2!.

%\subsection{Natural Language Understanding}
\subsection{Large Language Models and Generative AI}
A \ac{LLM} is a \ac{ML} model specifically designed for advanced \ac{NLP} tasks. Most \acp{LLM} are developed to generate text in natural language. Therefore, those \acp{LLM} are a specific type of \ac{GenAI}. What sets \acp{LLM} apart from other \ac{NLP} systems is their ability to learn different tasks simultaneously. For example, the \acp{LLM} developed by OpenAI known as \ac{GPT} are not limited to a specific \ac{NLP} tasks, but can summarize text, answer questions, and translate sentences.
\cite{radford2019language}

The capabilities of the \ac{LLM} technology are mainly based on very large text corpora used as training data and on the "transformer" architecture for deep \acp{ANN} introduced in \cref{Transformer}. One of the key differences using the transformer architecture is the vector representation of words. While the Word2vec algorithm creates one vector for each word, in a transformer model the words are broken into smaller chunks that are converted into embeddings as vector representation. Therefore, one word can consist of multiple vectors, allowing better generalization.
When a \ac{LLM} is trained on an extensive and diverse text corpus or several text corpora, it will be able to perform well in various domains and tasks.
\cite{radford2019language,brown2020language}

In general, \acp{LLM} are the most advanced approach to natural language understanding systems due to their ability to handle text input and generate adequate output.
\acp{LLM} are of interest for Document Understanding, because their capabilities can be applied for \acf{IE} and especially \acf{NER} tasks. An \ac{LLM}-based Document Understanding system can extract relevant data like addresses, dates, and clauses. Furthermore, it can summarize documents consisting of several pages and works with input documents in several languages.
\cite{brown2020language,carbonell2021named}
%Introduce generative models like generative adversarial networks (GANs) and variational autoencoders (VAEs).
%Explore their applications in natural language processing (NLP) tasks, including text generation, summarization, and translation.

\section{Artificial Intelligence for Document Understanding}
\subsection{Machine Learning based Document Understanding}
\acl{AI} and \acl{ML} provide important tools and techniques for Document Understanding. For example, \ac{ML} can be used for \ac{OCR} as well as \ac{NLP} tasks. In this section, the foundations for the training of learning systems and the functioning of \acp{ANN} are discussed. Furthermore, different types of \acp{ANN} used for Document Understanding systems are introduced.

\subsection{Supervised Learning}
\label{supervised}
Supervised learning is a paradigm for \ac{ML} systems, where a model \textit{learns} to produce the desired output for a collection of input data. The application of supervised learning is also often referred to as \textit{training} of a \ac{ML} model. The goal is that the trained system is able to produce reasonable results for unseen data.
\cite{haykin2009neural,knudsen1994supervised}

In principle, supervised learning uses pairs of input data and labels indicating the desired output. The learning system is adjusted in a way to minimize the overall error for all training pairs.
Preceding the actual supervised learning, there are often data pre-processing steps such as the manual creation of \textit{labels} indicating the desired output for a specific input. The availability of such labeled data is essential for successful supervised learning.
\cite{haykin2009neural}

The training process varies depending on the specific learning algorithm.
Regression techniques like \textit{linear regression} are among the simplest examples of supervised learning. The \acp{ANN} introduced in the following section of can be regarded as more advanced applications of the supervised learning paradigm. 
\Cref{pic:supervised} illustrates the supervised learning paradigm for \ac{ANN}. The network calculates the output for the given input. The output is compared to the target result. The error is translated into an instructive signal that adjusts the network. 
\cite{haykin2009neural,knudsen1994supervised}

\begin{figure}[ht]
    \centering 
    \includegraphics[width=0.6\textwidth]{pictures/supervised_learning.png}
    \caption{Components of the supervised learning paradigm \cite{knudsen1994supervised}}
    \label{pic:supervised}    % label the image for internal referencing
\end{figure}

When a supervised model learns the training data too well, including the noise and outliers, it performs poorly on new, unseen data, because it has not learned the underlying pattern, but the structure of the training data. This effect is called "overfitting".
\cite{haykin2009neural,knudsen1994supervised}

\subsection{Artificial Neural Networks}
\subsection*{From biological to artificial neural networks}
An \acf{ANN} aims to imitate biological neural networks like the human brain. The human brain consists of billions of cells called \textit{neurons}. Some neurons are connected with each other to form a network. Those connectors are called \textit{synapsis}. According to common theories, all memorized information is encoded in the network of neurons and synapses. Neurons communicate through an electro-chemical signaling system. To transmit information, a neuron releases a small discharge of electrical current to a part of the cell called \textit{axon}. When the axon is stimulated, it releases \textit{neurotransmitters}, which are signaling chemicals. 
A neuron is believed to have two states. Either it is at \textit{rest} or it \textit{fires}, which means that neurotransmitters are released. 
Neurotransmitters can have different effects on neurons. Some increase or decrease the probability that the receiving neuron will fire and others can potentiate the effect of other neurotransmitters.

Because neurons only have two states, in an \ac{ANN} they can be modeled as switches. Whether the artificial neuron fires or remains at rest can be modeled by an \textit{activation function}. In case of the simple perceptron the activation function is a discrete step function that returns either 1 or 0. The synapses are modeled as mathematical matrices in an \ac{ANN} and called \textit{weights}.
\cite{haykin2009neural,knudsen1994supervised}

\subsection*{Simple perceptron}
\begin{figure}[ht]
    \centering 
    \includegraphics[width=0.5\textwidth]{pictures/perceptron.png}
    \caption{Simple perceptron with three inputs and and one output node \cite{nielsen2015neural}}
    \label{pic:perceptron}    % label the image for internal referencing
\end{figure}
Inspired by the functioning of the human brain, the idea to encode information in neural networks has lead to the development of the "perceptron". The perceptron is a supervised learning algorithm for binary classification. In other words, the perceptron decides whether an input vector is part of a specific class or not. The name perceptron comes from the term perception, because the algorithm models the ability to perceive something. 

The first computational perceptron has been implemented by Frank Rosenblatt in 1957 \cite{Rosenblatt_1957}.  It consists of one layer of input neurons and one layer of output neurons connected by a layer of weights. Because of this, it is also called \textit{single-layer} perceptron opposed to the more powerful \textit{multi-layer} perceptrons that have additional \textit{hidden} layers. \Cref{pic:perceptron} shows a schematic representation of a simple perceptron.

A perceptron can be trained using the supervised learning paradigm. The learning rule of the simple perceptron follows the following steps:

\begin{enumerate}
    \item The weight matrices are initialized to random values.
    \item The input vectors are presented at the input layer.
    \item The network output is calculated.
    \item At each node of the output layer the error is calculated by comparing the network output with the expected result.
    \item The weights are updated with the value of the error.
\end{enumerate}

The steps from 2. are repeated until the overall error is below a defined threshold. It can be noticed the values of the weights converge to a constant state.
\cite{haykin2009neural,Rosenblatt_1957}

This single-layer perceptron can only solve problems of \textit{linear} nature. For example, it is able to model the behavior of the binary \textit{and}-operator and \textit{or}-operator. However, it is only possible to simulate the behavior of the \textit{xor} operator with a multi-layer perceptron and not with a single-layer perceptron. Nevertheless, it demonstrates that it is in principle possible to create an \ac{ANN}.
\cite{haykin2009neural}

\subsection*{Multi-layer Perceptron and Backpropagation}
\begin{figure}[ht]
    \centering 
    \includegraphics[width=0.7\textwidth]{pictures/mlp.png}
    \caption{Multi-layer perceptron with two hidden layers \cite{nielsen2015neural}}
    \label{pic:mlp}    % label the image for internal referencing
\end{figure}
Due to the limitations of the simple perceptron, the multi-layer perceptron has been developed. The multi-layer percepton consists of one or more additional \textit{hidden} layers. The weights connect the input and output nodes with the nodes of the hidden layer. This architecture is illustrated in \cref{pic:mlp}.

Because of the additional hidden layers, it is not as straightforward anymore to decide which weights to update. The common way is to use \textit{backpropagation} to update the weights. The backpropagation algorithm calculates the \textit{gradient}, which is a vector that indicates how much each weight should be changed to minimize the average error for the current training set or a subset of it called \textit{batch}.
\cite{nielsen2015neural,hochreiter2001gradient}

The backpropagation algorithm requires the activation function to be differentiable and therefore continuous. Most multi-layer perceptrons use a sigmoid function, because it is the closest continuous resemblance of a discrete step function.
\cite{haykin2009neural,nielsen2015neural}

Just like the simple perceptron the multi-layer perceptron is trained using the supervised learning paradigm. However, the general learning rule of the multi-layer perceptron consists of more steps including backpropagation:

\begin{enumerate}
    \item The weight matrices are initialized to random values.
    \item The input vectors are presented at the input layer.
    \item The network output is calculated.
    \item Propagate the error backwards through the network.
    \item Repeat the steps from 2. for all data of the current training set or batch.
    \item Calculate the average error and update the weights according to the gradient.
    \item Randomly shuffle the training set and create new batches to prevent that the order of the training data influences the network.
    \item Repeat the steps from 2. until the gradient crosses the defined threshold.
    \cite{nielsen2015neural}
    
\end{enumerate}

The multi-layer perceptron is the basic concept for any so-called \textit{feedforward} neural network, characterized by a forward information flow from the input to the output layer. This is considered to be the foundation for deep learning techniques. The specialized \acp{ANN} introduced in the following sections are all based on the architecture of the multi-layer perceptron.
\cite{nielsen2015neural}

\subsection{Convolutional Neural Network}
The \ac{CNN} is an advanced version of the multi-layer perceptron that is specialized for image processing. Because classical \acp{ANN} require the input to be in form of a vector, it is not directly possible to pass an image as input. On the other hand, \acp{CNN} are designed to handle 2D matrices (e.g. grayscale images) or 3D matrices (e.g. color images).
\cite{Goodfellow-et-al-2016}

The basic concept for \acp{CNN} has been introduced by the Japanese researcher Fukushima Kunihiko in 1980 under the name \textit{neocognitron} \cite{fukushima1980neocognitron}.
Just like the perceptron the \ac{CNN} is inspired by biological processes. In particular, by the neuron connectivity in the \textit{visual cortex}, the area of the brain responsible for visual information. 

The architecture of a \ac{CNN} consists of several layers with different purposes:
\begin{itemize}
    \item The \textit{Input Layer} holds the input data as a matrix (e.g. the pixel values of an image)
    \item The \textit{Convolutional Layer} performs the mathematical operation of a \textit{convolution} between the weights and small region of the input. A convolution has a similar effect like the application of a filter. This is done to highlight or flatten certain features of the input.
    \item The \textit{Activation Layer} applies the activation function elementwise.
    \item The \textit{Pooling Layer} performs a \textit{subsampling} operation along the height and width of the input. This results in a loss of information and is used to identify important features.
    \item The \textit{Fully-Connected Layer} computes the final results.
\end{itemize}
\begin{figure}[ht]
    \centering 
    \includegraphics[width=0.9\textwidth]{pictures/CNN.png}
    \caption{Architecture of a CNN for OCR \cite{lecun1998gradient}}
    \label{pic:CNN}    % label the image for internal referencing
\end{figure}

A \ac{CNN} typically consist of multiple arrangements of convolutional, activation, and pooling layers between the input and fully-connected layers. The effects of convolutions and subsampling are illustrated in \cref{pic:CNN}. A \ac{CNN} is trained using backpropagation. The update of the weights not only influences the flow of information, but also influences the \textit{filters} applied in the convolutional layer.
\cite{fukushima1980neocognitron,lecun1998gradient,Goodfellow-et-al-2016}

The primary application area for \acp{CNN} is image processing. In the context of Document Understanding this is especially interesting for \ac{OCR}.
In addition, \acp{CNN} can be useful for \ac{NLP} as well. For example, with them spatial relationships between words in a text can be captured. For Document Understanding, a \ac{CNN} can detect a relationship between the layout and the content of a document class.
\cite{lecun1998gradient,leiAI}

\subsection{Recurrent Neural Network and Long Short-Term Memory}

\acp{RNN} have been developed for the purpose of handling \textit{time-series} data especially text. In the context of \ac{NLP} time-series data means that the order of the words and sentences plays an important role for the meaning of a text. 
The main idea is that the output of a hidden network layer at time \textit{t} is stored in a "short-term memory" and will be used as additional input at time \textit{t+1}. Like this, the model can remember the previously predicted result providing more context and use it to predict the next one. \cite{hochreiter1997long,gers2000learning}

However, classical \acp{RNN} are affected by the so-called "vanishing gradient problem", where the gradient used to update the weights gets weaker and weaker with every iteration until it is essentially zero. Vice versa, it is also possible that the gradient grows exponentially instead of converging to zero. This makes it difficult to train an \ac{RNN} using backpropagation. This effect prevents the network to learn rather subtle patterns in the data. \cite{hochreiter2001gradient}

To deal with the issue of the "vanishing gradient problem" in classical \acp{RNN} the idea of the \ac{LSTM} has been introduced by Hochreiter and Schmidhuber of the Technical University Munich \cite{hochreiter1997long}. 

\ac{LSTM} is a type of \ac{RNN} that can distinguish between relevant and less relevant information. To achieve this, an \ac{LSTM} contains several \textit{forget gates} within the hidden layers at which it is determined which inputs should be placed in memory and what should be \textit{forgotten}. The \ac{LSTM} architecture is illustrated in \cref{pic:LSTM}.

\begin{figure}[ht]
    \centering 
    \includegraphics[width=0.7\textwidth]{pictures/LSTM.png}
    \caption{Three layer LSTM with memory blocks in hidden layer \cite{gers2000learning}}
    \label{pic:LSTM}    % label the image for internal referencing
\end{figure}

An LSTM-based ML model can learn that certain expressions are statistically more likely to follow a given expression than others. For example, in the context of \ac{NLP}, grammatical patterns can be detected and learned that way.
\cite{hochreiter1997long,hochreiter2001gradient}

\subsection{Transformer Architecture}
\label{Transformer}
Transformer Models are used for \ac{GPT} which are the technology powering ChatGPT \cite{brown2020language}.
\cite{vaswaniattentionneed}

\section{Business Applications of Document Understanding}
\subsection{Robotic Process Automation with integrated Document Understanding}
\ac{RPA} is a technology for the automation of business processes and is based on so-called \textit{software robots} and \ac{AI}. \ac{RPA} tools operate on the user interface level of an information system and mimic the actions of a human worker. 
\cite{van2018robotic}

Document Understanding can be combined with \ac{RPA} tools to automate document-based business processes such as invoice processing, order fulfillment, insurance form processing, and processes involving legal documents. For example, in the case of invoice processing a \ac{RPA} tool can forward the received invoices to a Document Understanding system that extracts relevant data like the vendor, order number, payment amount, and invoice date from the document. The extracted data can be returned to the \ac{RPA} tool that performs the necessary steps in the \ac{ERP} system.
\cite{leiAI,van2018robotic}

\subsection{When not to use Document Understanding}
When discussing the benefits of Document Understanding for \ac{RPA} also alternative approaches have to be considered. Document Understanding is especially useful for handling extensive documents like contracts and legal documents.
The previous example of \ac{RPA} and Document Understanding for invoice processing can be considered as an overelaborate solution. \cite{leiAI}

A more efficient approach to automate the invoice receipt workflow works without \ac{AI} and relies on standards of electronic data transfer. In this example, the vendor sends the invoice electronically in a machine-readable format. This replaces the traditional process, where the invoice is send as a printed or PDF document. An electronic invoice (e-invoice) is either issued via \ac{EDI} or as structured data object in XML format. \cite{alt2002integrierte,XRechnung}

With an \ac{EDI} the invoice data can be directly entered in the recipients \ac{ERP} system. However, this requires an organizational agreement between both parties. Therefore, an \ac{EDI} is usually only established between long-term business partners. \cite{alt2002integrierte,au2001should}

In this aspect the XML-based approach is more flexible and can be used without previous agreements. Nevertheless, all e-invoices need to be compatible to a defined standard. Such standards exist like the "XRechnung" data model for e-invoices in Germany. Furthermore, starting in 2025 it is mandatory for all companies in Germany to be able to send and receive e-invoices conforming to the "XRechnung" standard. \cite{XRechnung}

The e-invoicing approach is a more efficient and reliable solution for automated invoice processing and makes the implementation of a Document Understanding system obsolete. E-invoices have the advantage to be readable by humans and computers using a simple parsing technique.
\cite{alt2002integrierte,au2001should}