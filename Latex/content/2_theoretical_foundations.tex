\chapter{Theoretical Background}

\section{Document Understanding}
\subsection{Definition and Overview}
In general, Document Understanding is the process of ...
Traditionally, Document Understanding has been mainly concerned with the conversion of paper-based documents into a format that is readable for a computer \cite{taylor1994integrated}. 
With advancing digitization, Document Understanding covers the processing of digitally created documents as well. 

Traditionally, Document Understanding consists of an image processing step followed by a \ac{NLP} step. For both steps rule-based as well as \ac{ML} techniques exist. Paper-based documents start the Document Understanding process as scanned hard-copy of printed documents. Because these scans are essentially images representing the document at hand, in the first step the corresponding text has to be determined in a process called \ac{OCR}. During the image processing step the scanned document is converted from an image document format into a text document format. In the \ac{NLP} step the obtained text is processed to extract relevant information.

Digitally created documents already contain the information as text elements. Therefore, in principle the image processing part can be skipped. However, a different form of \ac{OCR} can still be helpful by providing layout information about the document. This is useful, because certain information tend to be more likely to be found is some regions of the document.

In summary, \ac{OCR} is needed extract the text from document images and to transform them into unstructured text.

\ac{IE} and its sub-task \ac{NER} are used to extract relevant information from unstructured text and transform documents into a logically structured data representation.

In the following, the three central Document Understanding methods \ac{OCR}, \ac{NER}, and \ac{IE} are introduced. For each technique the history, application, and advancements through \ac{ML} are presented.
%Review traditional methods like optical character recognition (OCR), named entity recognition (NER), and information extraction (IE).
%Discuss recent advancements in machine learning and deep learning for document understanding.
%Analyze the strengths and weaknesses of existing approaches.
\subsection{Optical Character Recognition}
\ac{OCR} is the process of converting images of text into machine-encoded text for example as ASCII code. In principle, \ac{OCR} can be thought of as a computer "reading" the text contained in an image. \cite{eikvil1993optical}
Because of the various use cases that machine reading offers, \ac{OCR} has been of interest for researches for several decades. In the 1960's the first commercial \ac{OCR} machines have been released. However, theses systems have been limited to a set of standardized fonts, whose distinctive shapes have been automatically matched to the corresponding letter. 
With advances in computer hardware and and easier access to computing resources, the first \ac{OCR} systems have been released as software in the 1970's. 
The challenge for any \ac{OCR} system is to provide an accurate recognition of printed as well as hand-written text.
Typically, in traditional\ac{OCR} systems the process starts by scanning the physical document to create a digital representation. Followed by locating the regions where text can be found. In the identified areas the text components are isolated from the rest of the image. When the scanned image contains noise for example due to a low resolution or errors from the segmentation of the image, pre-processing is necessary to increase the character recognition accuracy. Pre-processing can include applying filters to smooth the edges of the digitized characters and normalization to get characters of similar size and orientation. For the actual identification of the character various different approaches exist. This part is considered as the biggest challenge of \ac{OCR}. While early systems relied on template character matching techniques which are sensitive to noise and variation, more advanced techniques use mathematical transforms (e.g. Fourier transform) of the pixel values to discover identifying characteristics. The most advanced approach is based on neural networks that have been trained with a wide range of examples to recognize patterns. When all characters and symbols are converted in final step they will be grouped together into strings based on their positions and distances. Finally, some errors can be detected and corrected by checking for incorrect spelling with a underlying dictionary.
\cite{eikvil1993optical,nagy1999optical}

Generally, \ac{OCR} is known as a rather unreliable technology and to be prone to a wide range of errors. Especially, low resolution or low contrast can lead to a higher error rate. Also, some \ac{OCR} cannot always distinguish between very similar looking characters (e.g. the letter O and the number 0). Symbols appearing less frequently are also more likely to result in errors. Finally, special typographic features like unusual fonts can cause errors as well.
\cite{nagy1999optical,baird2004robust}

To improve \ac{OCR} robustness and accuracy a number of process steps can be considered. On the one hand, advanced image processing methods can be adapted to reduce the noise in the image during preprocessing. On the other hand, in the context of Document Understanding it can be taken advantage of the observation that most documents are created in a single font. Therefore, systems can be trained specifically and adapted for the kind of document the system is supposed to handle most effectively.
A more general, improvement is concerned with the approach of recognizing multiple characters instead of one by one. The concept behind this approach is that some words have a characteristic shape themselves and and the detected words can be matched to a context specific dictionary. This concept in turn, is the foundation of another approach as well, where the system is provided with the linguistic context, because it can be noticed that specific terms are more likely to be present in an invoice document than in a user manual for instance. \cite{nagy1999optical,baird2004robust}

The Tesseract software developed by HP and Google has been considered as one of the biggest advancements and is regarded as one of the most reliable \ac{OCR} systems of the 2000's. Tesseract is based on a \ac{LSTM} neural network that enables the system to "remember" the already processes characters to make more accurate predictions for the following character. \cite{Tesseract}

\subsection{Information Extraction}
\ac{IE} is concerned with automatically extracting information from unstructured or semi-structured machine readable documents. \ac{IE} aims to transform unstructured text into structured data to enable computational analysis and automated reasoning. To achieve this \ac{IE}-systems represent information written in natural language as a structured database. By extracting the logical form of the input text, \ac{IE} makes the identification of entities, relationships, and their semantic meanings within a specific domain possible. \ac{IE} is based on concepts of computer linguistics and usually uses methods from the related field of \acl{NLP}. In the context of Document Understanding, \ac{IE} plays an important role, because most documents contain mainly unstructured information and can therefore not be directly processed by machines. \cite{hobbs2010information,church1995commercial}

Just like for \ac{OCR} the roots of \ac{IE} can be traced back until the 1960's, where the first concepts of template-based \ac{IE} systems have been published. The series of seven \ac{MUC} between 1987 and 1998 are considered as a main driver behind the development of \ac{IE}. The \ac{MUC} have been focused on extracting information from news articles in a specific domain. The final two \acs{MUC} focused on \acf{NER}, which is an important sub-task of \ac{IE}. The goal of \ac{NER} is to identify meaningful objects in a text.
\cite{hobbs2010information,cowie1996information,black1998facile}

The most basic \ac{IE} approach is based on templates. An example for the template-based approach is the extraction of information that occur in a constant sequence like e-mail addresses or dates using a regular expression as template. This approach however, is quite limited if confronted with rather vague information that does not exactly fit in the categories of the underlying template. Another central challenge of \acf{IE} is the automated decision which information is relevant and which parts can be ignored without loosing important details. For those reasons more advanced techniques from \ac{NLP} are required for more robust results.
\cite{Wilks1997InformationEA,cowie1996information}

A promising approach is the idea of so-called learning \acf{IE} systems that do not rely on set in stone templates but use either statistical techniques or \ac{ML} algorithms. 
\cite{hobbs2010information,cardie1997empirical}

\subsection{Named Entity Recognition}
A "named entity" is the name a real-world object. Named entities can be names of persons, organizations, products, locations, etc..
\acf{NER} is concerned with identifying named entities in text and assigning them to their corresponding category. For instance, in the sentence "Steve Jobs was the founder of Apple Inc. and inventor of the iPhone" three named entities can be identified. In particular, "Steve Jobs" as a person entity, "Apple Inc." as an organization entity and finally "iPhone" as a product entity. \ac{NER} is of central importance in the field of Document Understanding, because some of the relevant information in documents fall into the category of a named entity. \cite{hobbs2010information,black1998facile}

For \ac{NER} to be successful an initial knowledge base also known as corpus in \ac{NLP} has to be set up. Typically, the corpus contains the name and type of the entity as well as information on the relation between two entities. For example, the information contained in the sentence "Mark Zuckerberg is the CEO of Meta Inc." would be stored as triplet: 

\{Person: Mark Zuckerberg, Relation: CEO, Organization: Meta Inc.\}.

A related challenge in this context is the so-called "coreference resolution", which is concerned with the case when there are multiple names for the same entity. For example, "Volkswagen" and its abbreviation "VW" should be linked to the same organization entity.
\cite{cowie1996information,cardie1997empirical}

For the 7th \ac{MUC} Black et al.\cite{black1998facile} developed a rule-based \ac{NER} system.

Besides its rigid nature, another major disadvantage of the rule-based approach is the labor intensity that goes into the manual definition of all rules. A suggested alternative are learning \ac{NER} systems that use \ac{ML} techniques and neural networks instead of set in stone rules.
\cite{hobbs2010information}

\newpage
\section{Natural Language Processing}
\subsection{Overview and History}
\ac{NLP} is concerned with the computer-based processing of information written in natural language. Typical tasks of \ac{NLP} are text classification, natural language understanding, and generation. \ac{NLP} is used for machine translation, text summarization, and chatbots.In the context of Document Understanding, \ac{NLP} provides important tools and techniques for \acf{IE}. \cite{aiello2002document}

For the first time, the automated processing and generation of natural language have been mentioned by Alan Turin in 1950 \cite{Turing1950}. Since then, \ac{NLP}-systems evolved from the initial rule-based approach to statistical concepts and finally using \ac{ML} and \acfp{ANN}. Before the 1980s, most \ac{NLP} systems relied on complex hand-written rules. Up until the early 2000s, statistical models have been the dominant method for \ac{NLP}. The first notable breakthrough of neural \ac{NLP} happened in 2003, when Benio et al. \cite{bengio} outperformed the state-of-the-art statistical language models using an \ac{ANN}.

Independent from the technical approach, any \ac{NLP}-system is based on a context specific text corpus. Text corpora are structured datasets containing large amounts of texts in a digital format. Usually, a text corpus is organized in a way to be easy to analyze and contains mainly test samples from real-world texts. For instance, academic journal repositories and Wikipedia can be seen as examples for text corpora. With a text corpus, it is possible to gain information about word and letter frequencies and patterns. These information are important for the construction or training of an \ac{NLP}-system.
\cite{church1995commercial,vzivzka2019text}

\subsection{Statistical NLP}
Statistical \ac{NLP} models aim to learn the probability of specific sequences of words that occur in the underlying text corpus.
The traditional statistical approach is based on so-called n-grams, which are instances of words that are commonly grouped together. Examples for bi-grams (\textit{n = 2}) are "Artificial Intelligence" or "New York". "Natural Language Processing" is an example of an n-gram, where \textit{n = 3}. N-gram models create probability tables for the next word given the current word. Disadvantages of this approach are that n-gram models cannot capture relations between words that are outside of the current window of size \textit{n} and therefore cannot deal with unseen word pattern. For small values of \textit{n} this approach is computationally efficient. However, as \textit{n} grows, the number of potential n-grams follows an exponential growth. This effect is know as "curse of dimensionality" and can be mitigated using \acp{ANN}
\cite{bengio,vzivzka2019text}

\subsection{Neural NLP}
Because \ac{ML} algorithms and \acp{ANN} cannot directly process letters, words, or sentences, the text has to be converted into numbers. This process called "tokenization" has several advantages. On the one hand, it leads to a compression of the size of the dataset. On the other hand, it is possible to encode the semantical meaning into the tokenized representation.
The advantage of \ac{NLP} models using an \ac{ANN} is that they can learn the probability of word sequences in parallel with the syntactic and semantic relationships between words. This can be achieved by representing each word as a vector. If two vectors are similar, this indicates a relationship between the words represented by them. With this approach vectors of words sharing the same word-stem will be similar. Also, words appearing in similar contexts like \textit{go} and \textit{walk} will have high similarity measures.
\cite{bengio,vzivzka2019text}

A widely adopted method for obtaining a vector representation for words the "Word2vec" technique developed at Google by Mikolov et al. \cite{mikolov2013}. For this technique an \ac{ANN} is trained with a large text corpus. The "Word2vec" algorithm maps each word in the text corpus to a specific vector that can have several hundred dimensions.

The resulting word vectors can then be used again as input for specialized \acp{ANN} for example for the purpose of machine translation and summarization.

\subsection{Natural Language Understanding}
\subsection{Large Language Models and Generative AI}
A \ac{LLM} is a \ac{ML} model specifically designed for advanced \ac{NLP} tasks. Most \acp{LLM} are developed to generate text in natural language. Therefore, those \acp{LLM} are a specific type of \ac{GenAI}. What sets \acp{LLM} apart from other \ac{NLP} systems is their ability to learn different tasks simultaneously. For example, the \acp{LLM} developed by OpenAI known as \ac{GPT} are not limited to a specific \ac{NLP} tasks, but can summarize text, answer questions, and translate sentences.
\cite{radford2019language}

The functioning of the \ac{LLM} technology is mainly based on very large text corpora used as training data and on the "Transformer" architecture for deep \acp{ANN} introduced in \cref{Transformer}. On of the key differences using the Transformer architecture is the vector representation of words.  
When a \ac{LLM} is trained on an extensive and diverse text corpus or several text corpora, it will be able to perform well in various domains and tasks.
\cite{radford2019language,brown2020language}

\acp{LLM} are of interest for Document Understanding, because their capabilities can be applied for \acf{IE} and especially \acf{NER} tasks. In general, \acp{LLM} are the most advanced approach to natural language understanding systems due to their ability to handle text input and generate adequate output.
\cite{brown2020language,carbonell2021named}
%Introduce generative models like generative adversarial networks (GANs) and variational autoencoders (VAEs).
%Explore their applications in natural language processing (NLP) tasks, including text generation, summarization, and translation.
\section{Artificial Intelligence for Document Understanding}
\subsection{Supervised Machine Learning}

When a supervised model learns the training data too well, including the noise and outliers, it performs poorly on new, unseen data, because it has not learned the underlying patterns, but the structure of the training data. This effect is called "overfitting".
\subsection{Artificial Neural Networks}
\subsection{Long Short-Term Memory}
\subsection{Convolutional Neural Network}
\subsection{Transformer Architecture}
\label{Transformer}
Transformer Models are used for Generative Pre-trained Models (GPT)
\subsection{Machine Learning based Document Understanding}
%Discuss the potential benefits of generative AI for document understanding.
\section{Process Automation with Document Understanding}
\subsection{Automated Invoice Processing}
Document Understanding can be combined with \ac{RPA} tools.
\subsection{When not to use Document Understanding}
When discussing the benefits of Document Understanding for \ac{RPA} also alternative approaches have to be considered. Document Understanding is 